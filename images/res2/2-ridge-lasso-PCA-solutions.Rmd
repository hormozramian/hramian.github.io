---
title: "Day-2-ridge-lasso-PCA"
author: "Seth Flaxman"
date: "03/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, you will familiarize yourself with the maths underlying the ridge and lasso penalties. Then you will 
write code to investigate bias and variance in the context of ridge, lasso, and the elastic net. 
Finally you will investigate Principal Components Analysis (PCA) as an alternative dimensionality reduction method.

Make sure the ``glmnet`` package is installed:
```{r}
library(glmnet)
```

## Ridge and lasso 
Let us considered squared error loss, i.e. in linear regression.
The ridge penalty adds an $L_2$ norm to a loss function: 
$\sum (y_i - x_i\beta)^2 + \lambda \|\beta\|^2_2$

The lasso penalty adds an $L_1$ norm to a loss function:
$\sum (y_i - x_i\beta)^2 + \lambda |\beta|_1^1$

The loss function for ridge leads to the following optimization problem:

$\mbox{argmin}_{\beta} \sum (y_i - x_i\beta)^2 + \lambda \|\beta\|^2_2$

* Assume that $\beta \in R^p$ is a p-dimensional vector and rewrite the loss function with the $L_2$ norm using the vector elements
$\beta_1$, $\beta_2, \ldots, \beta_p$.

*Answer.*
$\mbox{argmin}_{\beta} \sum (y_i - x_i\beta)^2 + \lambda (\beta_1^2 + \beta_2^2 + \ldots + \beta_p^2)$.

* Similarly, write the loss function for the lasso using vector elements.

*Answer.*
$\mbox{argmin}_{\beta} \sum (y_i - x_i\beta)^2 + \lambda (|\beta_1| + |\beta_2| + \ldots + |\beta_p|)$.

* To minimize these loss functions, we need to find their gradients. Find the gradient of the ridge penalized loss function. Find the gradient of the lasso penalized loss function. What parameter are you taking the gradient with respect to?

*Answer.*
The original version of this answer had an error. See ``final-notes.pdf``.

## Bias vs. variance
Ridge and lasso introduce *bias* by shrinking the parameters in a model towards zero. Here is the code from the unbiased linear regression demo in class:

```{r}
result = NULL
for(i in 1:1000) {
  n = 50
  x = rnorm(n)
  u = seq(-5,5,.1)
  error = rnorm(n) * .5
  y = 1.5 * x + error
  yhat = function(beta,x) {
    return(beta * x)
  }
  squared.loss = function(residuals) {
    sum(residuals^2)
  }
  
  betahat = optimize(function(beta) squared.loss(y - yhat(beta,x)),interval=c(-1,3))$minimum
  
  result = rbind(result,data.frame(beta.optim = betahat, true.beta = 1.5))
}
hist(result$beta.optim,breaks=25)
abline(v=mean(result$beta.optim),col="blue",lwd=2)
abline(v=1.5,col="red",lwd=2)
mean(result$beta.optim)
var(result$beta.optim)
```

* Modify the code to include a ridge penalty. Consider a range of $\lambda$ values from 0 to 10. What do you notice?
```{r}
result = NULL
for(lambda in 0:10) {
  for(i in 1:1000) {
    n = 50
    x = rnorm(n)
    u = seq(-5,5,.1)
    error = rnorm(n) * .5
    y = 1.5 * x + error
    yhat = function(beta,x) {
      return(beta * x)
    }
    squared.loss = function(residuals) {
      sum(residuals^2)
    }
    
    # add a ridge penaltys:
    betahat = optimize(function(beta) squared.loss(y - yhat(beta,x)) + lambda * beta^2,interval=c(-1,3))$minimum 
    
    result = rbind(result,data.frame(beta.optim = betahat, true.beta = 1.5))
  }
  print(sprintf("%d: %.02f",lambda,mean(result$beta.optim)))
}
```

As expected, $\beta$ shrinks as $\lambda$ increases.

* Modify the code to include a lasso penalty.  Consider a range of $\lambda$ values from 0 to 10. What do you notice?
```{r}
result = NULL
for(lambda in 0:10) {
  for(i in 1:1000) {
    n = 50
    x = rnorm(n)
    u = seq(-5,5,.1)
    error = rnorm(n) * .5
    y = 1.5 * x + error
    yhat = function(beta,x) {
      return(beta * x)
    }
    squared.loss = function(residuals) {
      sum(residuals^2)
    }
    
    # add a ridge penaltys:
    betahat = optimize(function(beta) squared.loss(y - yhat(beta,x)) + lambda * abs(beta),interval=c(-1,3))$minimum 
    
    result = rbind(result,data.frame(beta.optim = betahat, true.beta = 1.5))
  }
  print(sprintf("%d: %.02f",lambda,mean(result$beta.optim)))
}
```

Again, $\beta$ shrinks as $\lambda$ increases. However, the decrease is not as drastic as with ridge. 

* Earlier you found the derivatives of the ridge and lasso penalties. How does each vary with $\beta$? What does this mean in terms of how much each penalty penalizes large vs. small values of $\beta$?

**Answer** The derivative of ridge is linear in $\beta$. The derivative of lasso is constant in $\beta$. This means that while lasso penalizes  large and small values equally, ridge penalizes large values more than small values. 

## Mouse genome dataset example with ridge, lasso, and the elastic net

Ridge and lasso decrease *variance* by making a model less complex. We will consider a mouse genome dataset to predict red blood cell count from SNPs.
You can read about
the dataset at https://www.sanger.ac.uk/science/data/mouse-genomes-project 

The dataset has 1522 observations and 10346 predictors. I've provided you with an 80% train and 10% test dataset. (I'll release the remaining
10% on Thursday.) If we consider just linear regression,
the problem is overdetermined, and we can find an infinite number of models which perfectly fit the data:

```{r}
load("data/mice.rdata") # loads X_train, X_test, y_train, y_test
# there are computational difficulties in addition to theoretical ones with fitting a model with so many predictors, so we randomly sample 1300
set.seed(1)
ii = sample(ncol(X_train),1300)
X_train_df = as.data.frame(X_train[,ii])
X_train_df$y = y_train
X_test_df = as.data.frame(X_test[,ii])
fit = lm(y ~ ., data=X_train_df)
plot(y_train,predict(fit)) # an almost perfect fit
plot(y_test,predict(fit,X_test_df)) # very bad fit---this is because our model has high variance
```

* Inspect the coefficients in the linear model. You can find their values using ``coef(fit)``. Do you notice any that are very large in magnitude?

```{r}
coefs = coef(fit)
coefs[order(-coefs)][1:20]
```

**Answer** Yes! Many are very large.

Now let us see how lasso and ridge serve to decrease variance by shrinking all coefficients (ridge and lasso) and zeroing some of them out (lasso).
Here is how to fit the elastic net (a mixture of ridge and lasso)
in the ``glmnet`` package. We will now consider all 10346 predictors.
```{r}

# I have set the lambda sequences for illustration, but glmnet can cleverly determine these for you, so in practice, it is better to leave out the lambda parameter, i.e. fit.ridge=glmnet(X,y,alpha=0)
fit.ridge = glmnet(data.matrix(X_train),y_train,alpha=0,lambda = seq(100,.01,length.out = 100)) 
fit.lasso = glmnet(data.matrix(X_train),y_train,alpha=1,lambda=seq(2,.01,length.out=100)) 
fit.elastic = glmnet(data.matrix(X_train),y_train,alpha=.5,lambda=seq(2,.01,length.out=100)) 

par(mfrow=c(1,3))
plot(fit.ridge,xvar="lambda")
plot(fit.lasso,xvar="lambda")
plot(fit.elastic,xvar="lambda")
```

* Use ``predict`` to get predictions. If you call:
```
yhat = predict(fit.ridge,data.matrix(X_test))
```
you will get a matrix of predictions corresponding to the predictions made at each point in the regularization path. 
If you allowed  ``glmnet`` to choose the values of $\lambda$, you can check them by typing:
```
fit.ridge$lambda
```

* Calculate a loss function (which one is appropriate for a linear model?) at each value of $\lambda$ for lasso, ridge, and the elastic net, using the training data. Repeat for the testing data. Plot both of these curves---these are the training and testing learning curves.
**Answer: we use mean-squared error (sum of squared error is fine too)**
```{r}
yhat.train = predict(fit.ridge,data.matrix(X_train))
yhat.test = predict(fit.ridge,data.matrix(X_test))
mse.train = colMeans((yhat.train - y_train)^2) # this is an R trick---if you subtract a vector from a matrix, it subtracts the vector as a column vector elementwise from each column of the matrix
mse.test = colMeans((yhat.test - y_test)^2) 
plot(fit.ridge$lambda,mse.train,ty="l",col="blue",ylim=c(range(mse.train,mse.test)),main="ridge")
lines(fit.ridge$lambda,mse.test,col="red")
legend("topright",c("train MSE","test MSE"),fill=c("blue","red"))

yhat.train = predict(fit.lasso,data.matrix(X_train))
yhat.test = predict(fit.lasso,data.matrix(X_test))
mse.train = colMeans((yhat.train - y_train)^2) # this is an R trick---if you subtract a vector from a matrix, it subtracts the vector as a column vector elementwise from each column of the matrix
mse.test = colMeans((yhat.test - y_test)^2) 
plot(fit.lasso$lambda,mse.train,ty="l",col="blue",ylim=c(range(mse.train,mse.test)),main="lasso")
lines(fit.lasso$lambda,mse.test,col="red")
legend("topright",c("train MSE","test MSE"),fill=c("blue","red"))

yhat.train = predict(fit.elastic,data.matrix(X_train))
yhat.test = predict(fit.elastic,data.matrix(X_test))
mse.train = colMeans((yhat.train - y_train)^2) # this is an R trick---if you subtract a vector from a matrix, it subtracts the vector as a column vector elementwise from each column of the matrix
mse.test = colMeans((yhat.test - y_test)^2) 
plot(fit.elastic$lambda,mse.train,ty="l",col="blue",ylim=c(range(mse.train,mse.test)),main="elastic")
lines(fit.elastic$lambda,mse.test,col="red")
legend("topright",c("train MSE","test MSE"),fill=c("blue","red"))

```


* What values of $\lambda$ give solutions that approach that of ordinary linear regression (without regularization)? For lasso, what
value of $\lambda$ corresponds to a model with no predictors? What are the predictions for this model?

**ANSWER** As $\lambda$ goes to zero, we recover linear regression without regularization. As $\lambda$ gets very large, lasso zeros out all of the predictors.
```{r}
fit.lasso$lambda[1:5] # the largest value is lambda = 2; this is our first prediction below:
yhat.train = predict(fit.lasso,data.matrix(X_train))[,1]
yhat.test = predict(fit.lasso,data.matrix(X_test))[,1]
table(yhat.train) # all values are the same!
table(yhat.test) # all values are the same!
```
With no predictors, we just have an intercept (bias) term and thus all of the predictions are the same.

## Principal Components Analysis

You can apply PCA in R using ``prcomp()``:

```{r}
ptm = proc.time()
pc = prcomp(X_train,scale=TRUE) # note that this takes about a minute on my laptop
proc.time() - ptm
pc.summary = summary(pc)
plot(1:100,pc.summary$importance[3,1:100 ],ty="l",ylab="Percent of variance explained")
```

* Let us check some of the mathematical properties of the principal components. PC1 should explain the most variance, followed by PC2, etc. Check this for the first 5 principal components. You can find them in the columns of ``pc$x``.
```{r}
data.frame(pc1=var(pc$x[,1]),pc2=var(pc$x[,2]),pc3=var(pc$x[,3]),pc4=var(pc$x[,4]),pc5=var(pc$x[,5]))
```
* The principal components should be orthogonal. Check that this is true for the first 3 principal components by taking dot products amongst them. Note that you will not get values that are exactly 0 as computations on a computer are always carried out approximately, to the level of machine precision. Check that this is true for all principal components using a matrix operation on ``pc$x``. 

```{r}
q = t(pc$x) %*% (pc$x)
diag(q) = 0
range(round(q,8))
```

* Use linear regression on the first d principal components and calculate training and testing curves for
d = 5, 10, 20, 50, 200. Which value of d gives the best performance on the test dataset? Train dataset?
To obtain the principal components for the test dataset, use ``predict()``. Think about what mathematical operation
this function must be performing. Here is some code to get you started:

**ANSWER** as discussed in class, ``predict()`` uses the transformation matrix $W$ to transform new training data. 
```{r}
for(d in c(5,10,20,50)) {
  X_train_pc = pc$x
  X_test_pc = predict(pc, newdata = X_test)
  X_train_df = data.frame(X_train_pc[,1:d],y=y_train)
  X_test_df = data.frame(X_test_pc[,1:d],y=y_test)
  fit = lm(y ~ ., data=X_train_df)

  print(sprintf("%d train: %.02f, test: %.02f",d,
                mean((predict(fit) - y_train)^2),
                mean((predict(fit,X_test_df)-y_test)^2)))
}
```

